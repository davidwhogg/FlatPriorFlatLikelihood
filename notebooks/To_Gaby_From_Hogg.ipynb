{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoVHzLYc4GlHsZSfojzQlI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/FlatPriorFlatLikelihood/blob/main/notebooks/To_Gaby_From_Hogg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Gaby: flat LF, flat priors, and yet peaked posterior\n",
        "\n",
        "## Authors:\n",
        "- **David W. Hogg** (NYU) (MPIA)\n",
        "\n",
        "## Projects:\n",
        "- Show that even in the simplest situation: Everything linear, everything Gaussian, and flat priors, the Bayesian posterior is peaked when the likelihood function is not.\n",
        "  - That is, even integrating a flat LF over a flat prior can give you a spurious peak. It's sweet! The fundamental reason is that in more than a few dimensions, any misalignment between the model degeneracy and the box edges will give you structure.\n",
        "\n",
        "## Bugs / To-do items:\n",
        "- Show that as the truth (`CENTER`) moves, the peak in the LF moves accordingly.\n",
        "- Explore the effect of dimensionality of the space.\n",
        "- Explore the effect of the dimensionality of the degeneracy.\n",
        "- Show plots that are aligned with the degeneracy.\n",
        "- Show that the problem is easy to diagnose by changing the limits of the prior.\n",
        "- Switch to a latin hypercube, maybe?\n"
      ],
      "metadata": {
        "id": "8kUwNyqAi3Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyRSIiW6i2Yh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters / choices\n",
        "D = 6 # dimensionality of the space\n",
        "M = 2 # dimensionality of the null space (exact degeneracy space)\n",
        "K = 12 # number of samplings to do\n",
        "N = 2 ** (11 + D // 3) # number of samples (number of models to check inside the cube per sampling)\n",
        "SIGMA = 0.35 # precision in the non-null directions\n",
        "print(D, M, K, N)"
      ],
      "metadata": {
        "id": "HGU2XGwHjiFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make some random numbers to define our Universe\n",
        "rng = np.random.default_rng(17)\n",
        "CENTER = 10. * rng.uniform(size=D)\n",
        "vecs = rng.normal(size=(D, D))"
      ],
      "metadata": {
        "id": "JvZ_hgADjGzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now orthogonalize to make a randomly oriented orthonormal coordinate system\n",
        "UNITVECS = np.zeros((D, D))\n",
        "for i in range(D):\n",
        "    veci = 1. * vecs[i]\n",
        "    for j in range(i):\n",
        "        uvecj = UNITVECS[j]\n",
        "        veci -= (veci @ uvecj) * uvecj\n",
        "    UNITVECS[i] = veci / np.linalg.norm(veci)"
      ],
      "metadata": {
        "id": "kaHjxMO-mTYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a LLF with an exact M-dimensional degeneracy\n",
        "def log_likelihood(points):\n",
        "    \"\"\"\n",
        "    ## Bugs:\n",
        "    - Only can take lists of points, not a single point.\n",
        "    \"\"\"\n",
        "    deltas = (UNITVECS[M:] @ (points.T - CENTER[:, None])).T\n",
        "    return 10.0 - 0.5 * np.sum(deltas * deltas, axis=1) / (SIGMA * SIGMA)"
      ],
      "metadata": {
        "id": "JywiJ2tkkSNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_center = np.round(CENTER)\n",
        "axis_labels = list(\"abcdefghkmnpqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ\")[-D:]\n",
        "print(plot_center, axis_labels)"
      ],
      "metadata": {
        "id": "G9QDUW9BnJF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a sampling in a cube\n",
        "points = 2. * rng.uniform(size=(K * N, D)) - 1. + plot_center[None, :]\n",
        "llfs = log_likelihood(points)\n",
        "print(points.shape, llfs.shape)"
      ],
      "metadata": {
        "id": "FuyHULzFnk4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIGSIZE = 3. # units?\n",
        "nx = np.ceil(np.sqrt(D)).astype(int)\n",
        "ny = np.ceil(D / nx).astype(int)\n",
        "fig, axs = plt.subplots(ny, nx, figsize=(FIGSIZE * nx, FIGSIZE * ny), sharey=True)\n",
        "axs = axs.flatten()\n",
        "for i in range(D):\n",
        "    ax = axs[i]\n",
        "    ax.axvline(CENTER[i], lw=1, color=\"k\", alpha=0.5)\n",
        "    foo, bar = np.histogram(points[:, i], weights=np.exp(llfs[:]), density=True)\n",
        "    ax.stairs(foo, bar, color=\"k\", alpha=1.0, fill=False, label=f\"p({axis_labels[i]}|data)\")\n",
        "    for k in range(K):\n",
        "        foo, bar = np.histogram(points[k::K, i], weights=np.exp(llfs[k::K]), density=True)\n",
        "        ax.stairs(foo, bar, color=\"k\", alpha=0.25, fill=False)\n",
        "    ax.legend(loc=8)\n",
        "for j in range(i+1, len(axs)):\n",
        "    axs[j].set_visible(False)"
      ],
      "metadata": {
        "id": "3Ghp7iDTq9KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nplot = 2 ** 11\n",
        "vmax = np.max(llfs)\n",
        "vmin = vmax - 5.0\n",
        "fig, axs = plt.subplots(D-1, D-1, figsize=(FIGSIZE * (D - 1), FIGSIZE * (D - 1)))\n",
        "print(axs.shape)\n",
        "for i in range(1, D):\n",
        "    for j in range(i):\n",
        "        ax = axs[i - 1, j]\n",
        "        ax.scatter(points[:Nplot, j], points[:Nplot, i], s=4, c=llfs[:Nplot],\n",
        "                   vmin=vmin, vmax=vmax, cmap=\"viridis_r\", alpha=0.75)\n",
        "        if i == (D - 1): ax.set_xlabel(axis_labels[j])\n",
        "        if j == 0: ax.set_ylabel(axis_labels[i])\n",
        "    for j in range(i, D - 1):\n",
        "        axs[i - 1, j].set_visible(False)"
      ],
      "metadata": {
        "id": "fidul5uyo5UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lj6DAMy-qBMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}